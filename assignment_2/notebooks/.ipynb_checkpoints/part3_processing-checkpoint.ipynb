{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '../part3')\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import train\n",
    "from dataset import TextDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_dir = '../summaries'\n",
    "checkpoints_dir = '../checkpoints'\n",
    "text_dir = '../part3/books'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(text_dir, 'vanity_fair.txt'), 'r') as f:\n",
    "    text = f.read()\n",
    "    text = ''.join([i if ord(i) < 128 else ' ' for i in text])\n",
    "    \n",
    "with open(os.path.join(text_dir, 'vanity_fair.txt'), 'w') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs_vanity_fair.txt_random_0.5_rmsprop_64_0.0_0.002.txt\n",
      "logs_vanity_fair.txt_random_1.0_rmsprop_64_0.0_0.002.txt\n",
      "logs_vanity_fair.txt_random_2.0_rmsprop_64_0.0_0.002.txt\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(summaries_dir):\n",
    "    if file.endswith(\".txt\") and 'vanity' in file and 'random' in file:\n",
    "        with open(os.path.join(summaries_dir, file), 'r') as f:\n",
    "            print(file)\n",
    "            text = f.readlines()\n",
    "            sentences = ''.join(text[8:]).split('<EOF>')[:705]\n",
    "            selected_sentences = [sentences[176 * i] for i in range(5)]\n",
    "            results[file] = selected_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logs_vanity_fair.txt_random_0.5_rmsprop_64_0.0_0.002.txt': ['pcY&bSVTTkQS4 and?SZ#* o! j;.A',\n",
       "  ' a comprinklest fellows of her',\n",
       "  '\\'s brandy-treaction.  \"He had ',\n",
       "  'Dobbin should be a man of her ',\n",
       "  'llingted the depresent travell'],\n",
       " 'logs_vanity_fair.txt_random_1.0_rmsprop_64_0.0_0.002.txt': ['#jx!%dYikK.vXonWIc(Qw\"@_AyeOwX',\n",
       "  'ved and discovered herse. His ',\n",
       "  'Unce\\nin him home and going, th',\n",
       "  '\\'s long money,\" reC.B.n that t',\n",
       "  'y must have dreads everything '],\n",
       " 'logs_vanity_fair.txt_random_2.0_rmsprop_64_0.0_0.002.txt': ['Ll7 0ydu;5l4P2EpX24C9FS 68ZrWR',\n",
       "  '(\"mEstfanciewhaply K\\nHIPIO*, L',\n",
       "  'SEDee?\" phere evendher\\nPz8M;.,',\n",
       "  '.\\nGazipaG:-book,\" we us: \\n\\nAzl',\n",
       "  'RNHCTC!\\nM9ETLD! gher.\"  DT2per']}"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pcY&bSVTTkQS4 and?SZ#* o! j;.A',\n",
       " 'Xtern an mattot the nhe whe to',\n",
       " '] the the hit of the sarner hi',\n",
       " 'n and as the out the soll and ',\n",
       " 'k stane of the sead in the had',\n",
       " 'qued the courted denows of the',\n",
       " 'Flong to her four she too has ',\n",
       " '?  She poor before that he was',\n",
       " '-enenceled all her man her one',\n",
       " 'Miss Crawley was over her comp',\n",
       " 'Pody to him the laughter, and ',\n",
       " 'Qe the granders no more great\\n',\n",
       " '] was whose serger and a plani',\n",
       " '*were the poor order at the ha',\n",
       " 'les of the stome his should sh',\n",
       " 'Do of the strented the most\\nco',\n",
       " 'with the ladies was not pails ',\n",
       " 'PHECRLT TERAIT INID was not ar',\n",
       " 'Sedley and brought that her so',\n",
       " 'quitions, and were want his hi',\n",
       " 'x the most of the down and dis',\n",
       " ') and at the friends for the s',\n",
       " '4 which the course of his soun',\n",
       " 'K the hand was long to her the',\n",
       " 'bridge which he would not his ',\n",
       " 'd and room with an appoon her ',\n",
       " ', the brother was her covered ',\n",
       " '* of the couple and when the l',\n",
       " 'l service of the satimpered to',\n",
       " '/ the world and say asked the ',\n",
       " 'assed her little mortory of a ',\n",
       " 'Lang and said, and her of her ',\n",
       " 'Becky was a disposite from the',\n",
       " 'Zast that partal since the con',\n",
       " 'which the husband who said, an',\n",
       " 'is his bent to have been or th',\n",
       " 'The provist her declare of the',\n",
       " 'Sedley was his mother to her s',\n",
       " 'You mean that his bleasure whe',\n",
       " '$ the parties of her servants ',\n",
       " 'ze the principe of his friends',\n",
       " 'Morling here, and the famous a',\n",
       " '1velect and before the state a',\n",
       " '_ and George had one a counten',\n",
       " 'all of the subject dual the mo',\n",
       " 's and gave when indignst that ',\n",
       " \"O'Dowd and his looked close wa\",\n",
       " 'Father was on the world to him',\n",
       " 'n the confess of the most cond',\n",
       " '\" the great arma had any dear ',\n",
       " '?  The presented to the proper',\n",
       " 'I think they looked to send he',\n",
       " '3 little thought and the cours',\n",
       " '; and the cherent and the rath',\n",
       " 'le in the waiter of the sweet ',\n",
       " 'Park and so ladies to the part',\n",
       " \"'s dear the character at the s\",\n",
       " 'ong as the prepared of the mor',\n",
       " 'ng the property little coat of',\n",
       " 'he could she came to the offic',\n",
       " \"'s offices of the barravest an\",\n",
       " ') to the most coats which the ',\n",
       " 'he terms and flowings to see t',\n",
       " 'for a travelling to his houses',\n",
       " 'lady with the wife, who was a ',\n",
       " 'The appears with the bed serve',\n",
       " 'Destive the common as the room',\n",
       " 'k the dear well of the school ',\n",
       " ', and who was a heart at the h',\n",
       " ':  and the baronets of the boo',\n",
       " 'I should you might have seen t',\n",
       " 'parents stopped and an is a gr',\n",
       " ',\" said Jos would not like a a',\n",
       " 'Rebecca and the house, and the',\n",
       " \"'s she had personas by the hum\",\n",
       " 'The sights of the child of a p',\n",
       " 'f his appeared the servants of',\n",
       " '# the room, and who had been s',\n",
       " '6erest have been five the part',\n",
       " ': more conversation that she s',\n",
       " \"'s love which is one of the de\",\n",
       " 'zent and the and table of the ',\n",
       " '3 of the family for back the r',\n",
       " '7 a latter of the fortes, and ',\n",
       " 'John may have wondered the fri',\n",
       " '3 beats and children in the sa',\n",
       " 'g his father, who was not, the',\n",
       " '% and parting the man, and the',\n",
       " '2\\n\\nI shall see her own, that t',\n",
       " \"'s nothing of the nowed in the\",\n",
       " \"Queen's Crawley was struggled \",\n",
       " ', and and seen with the partic',\n",
       " ' in a great morning of there w',\n",
       " '4 the since with the charmed t',\n",
       " '--the\\npark-book and how sent a',\n",
       " 'n a view, he was no beans that',\n",
       " 'at more out of the promises wh',\n",
       " '97 the delighter to drink the ',\n",
       " 'You could be fixed to see the ',\n",
       " 'y wrains.\\n\\n\"Why had the two hi',\n",
       " '5; and were a face and the hou',\n",
       " 'I was a guarding to the servan',\n",
       " '] his mother were so weeping t',\n",
       " 'be a good trash of her coach a',\n",
       " 'ment with a that the authors, ',\n",
       " '120ect good hands and self-coa',\n",
       " 'ch her morning, and she was th',\n",
       " 'E having interpisted out to th',\n",
       " 'jorness, and her son and looke',\n",
       " '_ that the scrable to hear and',\n",
       " '@ the gattered to her a charge',\n",
       " '--her own marry about the plac',\n",
       " 'Knarent are not to the more of',\n",
       " 'Crawley and so been a trouse h',\n",
       " '*ITIT IV  The campaign.  \"I ha',\n",
       " 'XIII\\n\\nAnd the fect to the good',\n",
       " '&ject to seat his grandfather ',\n",
       " 'xtance of the money and the ca',\n",
       " 'Wes she was dinner to the char',\n",
       " 'quent for the best of the hous',\n",
       " 'Oster\\nSermon of his carriage. ',\n",
       " 'me to her happy all the marria',\n",
       " 'E Crawley to a score behaving ',\n",
       " \".  I don't command the troops \",\n",
       " 'room, and how pleased the fort',\n",
       " 'the day to this breathrickendi',\n",
       " '3 with a principums, and her h',\n",
       " 'came and the rear court which ',\n",
       " 'The most committed not the hou',\n",
       " 'by the carriage and sir, and t',\n",
       " 'grass and face the shoulder th',\n",
       " 'for it be an eldest great gent',\n",
       " '9 the child to be conversation',\n",
       " 've which an old operaliable ga',\n",
       " '?\"\\n\\n\"Have she said, and he was',\n",
       " 'Amelia put the place to the gr',\n",
       " '; so a should see the sort of ',\n",
       " 'quorning of her own old back a',\n",
       " \"I will be don't pay himself to\",\n",
       " '5 and his little gardens and b',\n",
       " '7 would tried that the old lad',\n",
       " 'U as a little presence of the ',\n",
       " '7 and the box and short of the',\n",
       " '5, and the\\ncharming the manner',\n",
       " 'I than it was going to walk th',\n",
       " '&covent, and the parties and m',\n",
       " '5, the children of the lady at',\n",
       " '), which were the feelings of ',\n",
       " ': and the lady of the personag',\n",
       " 'on the advantage of the consol',\n",
       " 'll the advantage of the gentle',\n",
       " ':  and it was a little breakfa',\n",
       " 'Russel he had been the compane',\n",
       " '3 not in the bed establishment',\n",
       " '&  On the present of the ready',\n",
       " 'ver to the sisters and cousing',\n",
       " 'XTHE STU ONT CILDEMTER LIIIII ',\n",
       " '5 he came to her hand to her, ',\n",
       " 'Xant he was the mother sander ',\n",
       " 'jorned the son, not the party ',\n",
       " '# the side of the intelligant ',\n",
       " 're great family and position b',\n",
       " '4 the little way that when he ',\n",
       " 'He was a benches and a hands i',\n",
       " 'and for her to the window whic',\n",
       " 'Kits was a parties which was a',\n",
       " 'melias carried to him or marry',\n",
       " 'Emmy and a play of the Colonel',\n",
       " 'he place of the company of the',\n",
       " '* Sir Pitt and the house of th',\n",
       " 'ing and works, and she thought',\n",
       " '3 that he could have the very ',\n",
       " 'was off my dear.\\n\\n\"You know he',\n",
       " 'Quale Countess of a people of ',\n",
       " 'e happened with his grandfathe',\n",
       " 'You are to a good hands of the',\n",
       " ' a comprinklest fellows of her',\n",
       " 'No\\ndistinguish of the private ',\n",
       " 'ke you, you done he was a pers',\n",
       " 'y he was the son was supposed ',\n",
       " 've her sister and the faintanc',\n",
       " '_ the corner of his intervicti',\n",
       " \"Queen's Crawley, who was a pai\",\n",
       " 'me of the world.  \"I have the ',\n",
       " ') the garden of his father wou',\n",
       " '18 now, \"I have been to be to ',\n",
       " 'Why all the neighbour of the a',\n",
       " 'f the company of the companion',\n",
       " ', and the conduction came to d',\n",
       " 'U said, \"I should not take the',\n",
       " 'quire of the course of the gre',\n",
       " 't her consider and so pain and',\n",
       " 'Rebecca had been a sister in t',\n",
       " 'At a party, who could be on th',\n",
       " 'particularly to a great which ',\n",
       " 'less of his father was off to ',\n",
       " 'Kirth and my terms of the plea',\n",
       " '[est a state for the country t',\n",
       " 'part of the part of the little',\n",
       " ') to see her a place of the co',\n",
       " 's the charming promise of a me',\n",
       " '% the last pretty forestal com',\n",
       " ', and the works of the solitin',\n",
       " 'ked the shore of the particula',\n",
       " '#nerring and which he was in a',\n",
       " '3 to be reading in\\nthe manages',\n",
       " '&  I have he had been against ',\n",
       " ', to the satisfacter which the',\n",
       " 'reat and the latters in the pa',\n",
       " '4 the good morning about his a',\n",
       " 'Southdown and loved in the com',\n",
       " \"Mrs. O'Dowd of the strange, he\",\n",
       " 'Fate and shaking and some a\\nli',\n",
       " 'on the prospect of the girls a',\n",
       " 'When she sent him at the new a',\n",
       " 'bout that the mils of the ladi',\n",
       " '00, in a score which a former ',\n",
       " 'the dancer of the past of the ',\n",
       " 'f the days to come to her moth',\n",
       " '101, and the same two disappea',\n",
       " 's of the course of the house t',\n",
       " 'nd the hall for how she intere',\n",
       " '(she had the carriage of the s',\n",
       " 'jor had in the carriage and mi',\n",
       " 'me her heart to take his parti',\n",
       " '), and stored to the Colonel w',\n",
       " 'Prichual she would have the ne',\n",
       " 'Crawley were and the mate star',\n",
       " 'ked the cards of the half a co',\n",
       " '200, and the produce of the pr',\n",
       " 'He took the admirable front of',\n",
       " 'or a shoulder and the time to ',\n",
       " '] and had disposed to say his ',\n",
       " 'ng the poor pretty door, and t',\n",
       " 'Rebecca and the stach bed that',\n",
       " 'money was very dined to see th',\n",
       " 'Southdown of the appearance wh',\n",
       " 'Ummendows had been of the darl',\n",
       " 'fering to his wife and satin t',\n",
       " \"Jos's wife must be that a brea\",\n",
       " 'VITNING YOU be to the parishon',\n",
       " \"'s father and still for his be\",\n",
       " 'le the latter to his brother a',\n",
       " 'very common to her that depape',\n",
       " 'Colonel under her beloness and',\n",
       " \"on't be a money, and the young\",\n",
       " 't her thread of the ladies in ',\n",
       " 'Captain Sedley was a little on',\n",
       " 'x the first discoverer of the ',\n",
       " 'It was so grand and the brideg',\n",
       " '% the country of the wiss and ',\n",
       " '1500 in the establish passage ',\n",
       " 'quiet of a member of the remem',\n",
       " '?\" and it was after the poor c',\n",
       " \"99.  You've sure Mr. Osborne, \",\n",
       " '# had been\\ndistinguished to li',\n",
       " '20 before the see of the house',\n",
       " 'll of the foreother and passed',\n",
       " 'YOUR Macmurnificent army and m',\n",
       " '_ her servant the course and t',\n",
       " 'The side which he had to take ',\n",
       " '; how persons when the paragra',\n",
       " 'Vanity Fair of him pleased and',\n",
       " 'xpect the aunt of the great ge',\n",
       " 'urely liked that the sisters w',\n",
       " 'George was that for the other ',\n",
       " ':  and so and a pounds as ever',\n",
       " 'Vanity Sedley should be a litt',\n",
       " 'Oh, his own woman was at the y',\n",
       " 'was since the front of the fat',\n",
       " 'Vanity of the present state of',\n",
       " '7 that the scholar parting of ',\n",
       " '; and been not the world, when',\n",
       " \"*****C Crawley's poor moment w\",\n",
       " 'Gutenberg-tm and a man that he',\n",
       " 'o a dinner, he would have been',\n",
       " \"'s great books of the boy than\",\n",
       " '--the three some belief the ho',\n",
       " 'Lord Steyne, and said that he ',\n",
       " '/ had played to her that the l',\n",
       " 'party, and in the parents the ',\n",
       " '50, and the above officer with',\n",
       " 'jor that we be a ladyship was ',\n",
       " 'You look to his mother, who wa',\n",
       " 'xoll woman said as the ladies ',\n",
       " \"Crawley's game of the carriage\",\n",
       " \"George's mother was of the cha\",\n",
       " 'You like to be oced to the bel',\n",
       " '] the old many the companion o',\n",
       " '7 which had stopped the stairs',\n",
       " 'Amelia was the same for the ba',\n",
       " 'xisted to his in the service s',\n",
       " '[dinner, and he was in the dec',\n",
       " 'Jos could not how he had not t',\n",
       " 'Faith in any man, when he was ',\n",
       " 'ad for her husband, who held t',\n",
       " 'the world in the most beautifu',\n",
       " '!\" said the love to have a lit',\n",
       " \"Queen's Crawley.  I was made a\",\n",
       " 'o the defence\\nof the ladies pr',\n",
       " '#stand to take the course of t',\n",
       " \"? I've never seemed at him wit\",\n",
       " 'n the marriage, and were the b',\n",
       " '% when the other had been the ',\n",
       " 'ped, the Gove the specially of',\n",
       " 'Slighter had been as a claret ',\n",
       " 'or disted the great and an eve',\n",
       " 'ble of the Major, and she was ',\n",
       " 'd not that his dear man and pa',\n",
       " 'Crawley was another and the gr',\n",
       " 'g a confidence.  It was there ',\n",
       " 'Zuleest, and the father to her',\n",
       " 'Zutentance of a pride and a fa']"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.187293637137882"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(705000/(1735414/30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n",
      "trained_model_vanity_fair.txt_random_1.0_rmsprop_64_0.0_0.002.pth\n",
      "trained_model_vanity_fair.txt_random_0.5_rmsprop_64_0.0_0.002.pth\n",
      "trained_model_vanity_fair.txt_random_2.0_rmsprop_64_0.0_0.002.pth\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(checkpoints_dir):\n",
    "    print(file)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'trained_model_vanity_fair.txt_random_1.0_rmsprop_64_0.0_0.002.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('lstm.weight_ih_l0',\n",
       "              tensor([[ 4.7765e-01,  1.3042e-03, -6.8638e-01,  ..., -1.7391e+00,\n",
       "                       -2.1795e-01,  1.3463e-01],\n",
       "                      [-1.6811e+00, -1.2090e-01, -1.5511e+00,  ..., -2.7557e+00,\n",
       "                        9.4601e-01, -1.0742e-01],\n",
       "                      [-7.1630e-01,  8.1892e-03, -2.9258e-01,  ..., -1.2302e+00,\n",
       "                        6.7257e-02,  3.8162e-02],\n",
       "                      ...,\n",
       "                      [-2.3161e+00, -7.6396e-02,  6.2116e+00,  ...,  6.6898e-01,\n",
       "                       -1.8804e+00,  2.7601e+00],\n",
       "                      [ 6.7790e-02,  9.0006e-04,  1.1342e-01,  ..., -5.9251e-01,\n",
       "                        1.1991e-01, -1.7963e-01],\n",
       "                      [-4.1656e+00,  3.7631e-03, -4.7108e-01,  ..., -8.8465e-01,\n",
       "                       -1.4005e+00, -6.1340e-01]])),\n",
       "             ('lstm.weight_hh_l0',\n",
       "              tensor([[-4.9469e-01, -6.1933e-01, -9.0437e-02,  ...,  7.1701e-02,\n",
       "                        3.5977e-01, -5.7208e-01],\n",
       "                      [ 1.3412e+00,  1.4792e+00, -1.1128e+00,  ...,  1.2678e+00,\n",
       "                       -1.5316e-01,  1.6946e-01],\n",
       "                      [ 1.6099e-01, -6.1010e-01,  2.7551e-01,  ..., -1.6575e-01,\n",
       "                       -7.3432e-02,  1.1604e+00],\n",
       "                      ...,\n",
       "                      [ 6.7719e-02,  4.2222e-01,  8.1927e-01,  ..., -1.4678e+00,\n",
       "                       -6.3337e-01,  5.5565e-01],\n",
       "                      [-6.9502e-02, -7.2019e-01, -5.9957e-01,  ..., -1.0859e-01,\n",
       "                        5.9032e-01,  1.5840e+00],\n",
       "                      [ 2.2313e-01,  5.7036e-01, -9.9060e-01,  ...,  1.4625e+00,\n",
       "                       -7.9024e-01,  5.2079e-01]])),\n",
       "             ('lstm.bias_ih_l0',\n",
       "              tensor([-1.3421, -1.8089, -1.0506, -1.8168, -0.1688, -0.6698, -2.1254,\n",
       "                      -0.0598, -0.3986, -2.2844, -0.9660, -1.4122, -1.0944, -1.0382,\n",
       "                      -1.6031, -1.2381, -0.6600, -1.3604, -1.3234, -2.0466,  0.5083,\n",
       "                      -1.0496, -0.5334, -0.0213, -2.0049, -1.1442, -1.1522, -1.4499,\n",
       "                      -1.9754, -0.9377,  0.0744, -1.6415, -0.9897, -1.3364, -0.9123,\n",
       "                      -1.0601, -0.2702, -0.7436, -0.1139, -1.5468, -1.3593, -1.0266,\n",
       "                      -0.2785, -1.6371, -1.2654, -1.7557, -1.6527,  0.0020, -2.6837,\n",
       "                      -0.0617, -2.1635, -1.2789, -1.4207,  1.1622,  0.0127, -0.4516,\n",
       "                      -0.6613, -1.2965, -2.0765, -2.0611, -0.7504, -1.5347,  0.8777,\n",
       "                      -1.4356, -0.4278, -1.7256, -1.4824,  1.1264, -1.8610, -1.0353,\n",
       "                      -0.2776, -1.7273, -0.9584, -1.6158, -1.0083, -1.1402, -0.3046,\n",
       "                      -1.9792, -0.3246, -0.8583, -2.1903, -1.3788, -1.3141, -1.0531,\n",
       "                       0.3309, -0.1075, -0.9869, -1.9643,  0.3174, -1.2754, -0.1840,\n",
       "                      -0.9580,  0.2578,  0.1965, -1.4363,  0.4714, -0.5457, -3.2427,\n",
       "                      -0.9394, -2.3577, -1.0002, -2.1661, -1.7323, -1.2534, -0.4910,\n",
       "                      -0.4370, -0.3103,  0.2345, -1.4509, -1.4372, -0.7404, -1.7579,\n",
       "                      -1.2128, -2.1170, -0.6531, -0.6656, -0.9018, -2.1606, -2.3020,\n",
       "                      -1.8053, -0.0597, -1.5428,  0.2052,  1.1149,  0.4565,  0.3266,\n",
       "                      -0.7770, -0.0107, -0.1081, -2.7764, -1.2792, -1.3178, -2.9000,\n",
       "                      -2.3883, -0.7978, -1.8329, -0.2666,  0.4435,  0.9942, -0.5021,\n",
       "                      -1.2128, -2.4604,  0.3613, -0.7020, -1.7951, -0.6114, -1.1152,\n",
       "                      -1.1510, -2.1403, -0.9296, -1.2920, -1.5993, -0.3063,  0.0282,\n",
       "                      -1.5064, -0.6777, -0.8573,  2.3839, -1.6466, -0.3470, -0.6286,\n",
       "                      -0.8396, -0.6073, -0.8065, -1.2002, -0.7437, -1.4638, -0.2581,\n",
       "                      -0.8521, -1.8941, -1.0417, -0.9246, -0.5534, -0.8429, -1.3143,\n",
       "                      -0.4731,  1.3314, -2.5532,  0.5559, -1.2389, -0.2500, -1.4940,\n",
       "                      -1.9729, -1.0926, -0.4892, -1.2922, -0.2920, -0.6231, -1.2145,\n",
       "                      -0.6291,  0.4838, -0.5027, -0.8823,  0.3173, -0.4326, -0.9017,\n",
       "                      -0.9934, -0.5014, -2.2552, -0.7913, -0.7988, -1.2533,  0.3351,\n",
       "                       0.3286,  0.2085, -0.3109, -0.0035, -0.0800, -1.7648, -1.2436,\n",
       "                      -0.3181, -1.4417, -1.7192, -0.9152, -0.5507, -0.7551,  0.2887,\n",
       "                      -0.5246, -1.3323, -1.0619, -0.7114, -1.7316,  0.0205,  0.5901,\n",
       "                      -1.5392, -0.8391, -0.6482, -0.7117, -0.9683,  0.3870, -1.5636,\n",
       "                      -1.6492, -1.5304, -0.7023, -1.5287, -1.7805, -0.9419, -1.8413,\n",
       "                      -0.5360,  0.2382, -0.7469, -0.8510, -1.0132, -0.7300, -0.1267,\n",
       "                      -0.4524, -0.8601, -0.8758, -0.8267, -1.2733, -2.3915, -1.7589,\n",
       "                      -1.4758, -1.7482, -0.4346, -1.7467,  0.9090, -0.2886, -0.9880,\n",
       "                      -1.0331,  0.4684, -1.3066, -0.9889,  0.0111,  2.5996, -1.4322,\n",
       "                       1.1743,  1.1226,  1.1168, -0.8510, -0.9314, -0.7222, -0.3608,\n",
       "                       1.2388,  0.8192,  0.9784,  1.5673, -1.0559,  1.1557, -0.3685,\n",
       "                       0.8813, -1.8355,  0.0955,  0.6037, -0.9689, -0.4095,  0.2605,\n",
       "                       0.8611, -0.2033,  0.9501,  0.3226, -0.9821, -0.3645, -1.2440,\n",
       "                      -1.3034, -0.7386,  1.1962, -1.3889,  1.0964,  0.8687,  1.0274,\n",
       "                       1.1122,  1.2412,  1.5030, -1.4868, -0.7998, -1.1344,  0.4843,\n",
       "                       1.0549,  1.4025, -1.0876, -2.1512, -0.8254,  0.1436,  0.6996,\n",
       "                      -1.2837,  0.1265,  1.0358,  1.2790,  0.9407, -0.7309, -0.9407,\n",
       "                       0.8926, -1.3090, -0.8112,  0.6735, -0.7722, -1.3697, -0.7544,\n",
       "                       1.0743,  1.5304,  1.4486, -1.0604, -0.6614,  0.2102,  1.7119,\n",
       "                       1.4446, -0.8488, -1.1972, -1.8893, -0.4017, -0.4829,  1.3083,\n",
       "                       1.0836, -1.5890, -0.2127,  1.0742, -0.2562, -1.5217,  0.3588,\n",
       "                      -1.2009,  1.2818, -1.4474, -1.2169, -0.4000,  1.3599,  1.7833,\n",
       "                       0.9570, -0.5710,  0.4007,  0.0744,  0.0706,  0.1190,  0.3176,\n",
       "                       1.1248, -0.6395, -0.9588, -0.8709,  1.1888, -0.9069, -1.1781,\n",
       "                      -0.8922,  1.7925,  1.0113, -0.8882,  1.3649, -0.2139,  0.7986,\n",
       "                       0.2530, -1.0754,  1.1563, -0.1707, -0.8832,  0.4490, -1.2014,\n",
       "                       0.4467, -1.3346, -1.3749,  0.7060, -2.7703, -1.7270, -0.4251,\n",
       "                       0.1126, -1.8608, -2.5030, -1.6292, -1.8886, -0.1196, -1.2121,\n",
       "                      -1.1265, -0.6390, -1.9251, -1.2612, -0.6907, -2.2288, -1.7791,\n",
       "                      -1.7050, -0.6201, -2.8140, -1.8854,  0.7267, -1.6463, -1.6935,\n",
       "                      -1.0627, -0.8754, -1.5620, -0.6452, -2.9258, -0.5663, -2.3230,\n",
       "                      -0.3078, -2.8250, -1.9509, -1.7657, -2.6720, -0.1047, -2.0093,\n",
       "                      -1.9252, -1.3313, -1.5546, -2.0423,  0.0135, -0.5282,  1.3577,\n",
       "                      -1.9627, -0.6436, -1.2796, -2.7830, -1.1334, -0.3869, -1.5235,\n",
       "                      -0.6544, -1.3981, -2.1400, -0.9185, -1.8668, -3.6721, -3.2972,\n",
       "                      -1.4407, -1.6381, -1.8742, -3.4136, -0.7557, -0.7355,  0.1168,\n",
       "                      -1.5879, -2.1513, -1.9841, -1.9063, -2.6081, -3.0016, -1.0724,\n",
       "                      -1.6180,  0.5582, -2.7447, -1.3945, -2.0942, -2.0914, -1.0179,\n",
       "                      -1.0616, -3.1113, -1.0394, -3.5438, -0.7557, -2.8756, -0.5336,\n",
       "                      -2.4766, -0.3584, -2.5090, -3.8061, -1.0999, -1.5480, -2.2850,\n",
       "                      -2.4039, -2.5702, -1.2012, -1.0112, -1.4415, -0.5966, -1.3911,\n",
       "                      -1.2842, -0.8893, -2.5420, -1.5262, -1.1195, -1.7759, -2.3023,\n",
       "                      -1.3368,  0.0230, -1.6429, -2.4655, -1.7453, -1.8772, -1.5280,\n",
       "                      -1.0304, -1.5237,  0.3462, -0.4884, -1.4165, -0.5040, -1.1604,\n",
       "                      -0.4604])),\n",
       "             ('lstm.bias_hh_l0',\n",
       "              tensor([-1.3421, -1.8089, -1.0506, -1.8168, -0.1688, -0.6698, -2.1254,\n",
       "                      -0.0598, -0.3986, -2.2844, -0.9660, -1.4122, -1.0944, -1.0382,\n",
       "                      -1.6031, -1.2381, -0.6600, -1.3604, -1.3234, -2.0466,  0.5083,\n",
       "                      -1.0496, -0.5334, -0.0213, -2.0049, -1.1442, -1.1522, -1.4499,\n",
       "                      -1.9754, -0.9377,  0.0744, -1.6415, -0.9897, -1.3364, -0.9123,\n",
       "                      -1.0601, -0.2702, -0.7436, -0.1139, -1.5468, -1.3593, -1.0266,\n",
       "                      -0.2785, -1.6371, -1.2654, -1.7557, -1.6527,  0.0020, -2.6837,\n",
       "                      -0.0617, -2.1635, -1.2789, -1.4207,  1.1622,  0.0127, -0.4516,\n",
       "                      -0.6613, -1.2965, -2.0765, -2.0611, -0.7504, -1.5347,  0.8777,\n",
       "                      -1.4356, -0.4278, -1.7256, -1.4824,  1.1264, -1.8610, -1.0353,\n",
       "                      -0.2776, -1.7273, -0.9584, -1.6158, -1.0083, -1.1402, -0.3046,\n",
       "                      -1.9792, -0.3246, -0.8583, -2.1903, -1.3788, -1.3141, -1.0531,\n",
       "                       0.3309, -0.1075, -0.9869, -1.9643,  0.3174, -1.2754, -0.1840,\n",
       "                      -0.9580,  0.2578,  0.1965, -1.4363,  0.4714, -0.5457, -3.2427,\n",
       "                      -0.9394, -2.3577, -1.0002, -2.1661, -1.7323, -1.2534, -0.4910,\n",
       "                      -0.4370, -0.3103,  0.2345, -1.4509, -1.4372, -0.7404, -1.7579,\n",
       "                      -1.2128, -2.1170, -0.6531, -0.6656, -0.9018, -2.1606, -2.3020,\n",
       "                      -1.8053, -0.0597, -1.5428,  0.2052,  1.1149,  0.4565,  0.3266,\n",
       "                      -0.7770, -0.0107, -0.1081, -2.7764, -1.2792, -1.3178, -2.9000,\n",
       "                      -2.3883, -0.7978, -1.8329, -0.2666,  0.4435,  0.9942, -0.5021,\n",
       "                      -1.2128, -2.4604,  0.3613, -0.7020, -1.7951, -0.6114, -1.1152,\n",
       "                      -1.1510, -2.1403, -0.9296, -1.2920, -1.5993, -0.3063,  0.0282,\n",
       "                      -1.5064, -0.6777, -0.8573,  2.3839, -1.6466, -0.3470, -0.6286,\n",
       "                      -0.8396, -0.6073, -0.8065, -1.2002, -0.7437, -1.4638, -0.2581,\n",
       "                      -0.8521, -1.8941, -1.0417, -0.9246, -0.5534, -0.8429, -1.3143,\n",
       "                      -0.4731,  1.3314, -2.5532,  0.5559, -1.2389, -0.2500, -1.4940,\n",
       "                      -1.9729, -1.0926, -0.4892, -1.2922, -0.2920, -0.6231, -1.2145,\n",
       "                      -0.6291,  0.4838, -0.5027, -0.8823,  0.3173, -0.4326, -0.9017,\n",
       "                      -0.9934, -0.5014, -2.2552, -0.7913, -0.7988, -1.2533,  0.3351,\n",
       "                       0.3286,  0.2085, -0.3109, -0.0035, -0.0800, -1.7648, -1.2436,\n",
       "                      -0.3181, -1.4417, -1.7192, -0.9152, -0.5507, -0.7551,  0.2887,\n",
       "                      -0.5246, -1.3323, -1.0619, -0.7114, -1.7316,  0.0205,  0.5901,\n",
       "                      -1.5392, -0.8391, -0.6482, -0.7117, -0.9683,  0.3870, -1.5636,\n",
       "                      -1.6492, -1.5304, -0.7023, -1.5287, -1.7805, -0.9419, -1.8413,\n",
       "                      -0.5360,  0.2382, -0.7469, -0.8510, -1.0132, -0.7300, -0.1267,\n",
       "                      -0.4524, -0.8601, -0.8758, -0.8267, -1.2733, -2.3915, -1.7589,\n",
       "                      -1.4758, -1.7482, -0.4346, -1.7467,  0.9090, -0.2886, -0.9880,\n",
       "                      -1.0331,  0.4684, -1.3066, -0.9889,  0.0111,  2.5996, -1.4322,\n",
       "                       1.1743,  1.1226,  1.1168, -0.8510, -0.9314, -0.7222, -0.3608,\n",
       "                       1.2388,  0.8192,  0.9784,  1.5673, -1.0559,  1.1557, -0.3685,\n",
       "                       0.8813, -1.8355,  0.0955,  0.6037, -0.9689, -0.4095,  0.2605,\n",
       "                       0.8611, -0.2033,  0.9501,  0.3226, -0.9821, -0.3645, -1.2440,\n",
       "                      -1.3034, -0.7386,  1.1962, -1.3889,  1.0964,  0.8687,  1.0274,\n",
       "                       1.1122,  1.2412,  1.5030, -1.4868, -0.7998, -1.1344,  0.4843,\n",
       "                       1.0549,  1.4025, -1.0876, -2.1512, -0.8254,  0.1436,  0.6996,\n",
       "                      -1.2837,  0.1265,  1.0358,  1.2790,  0.9407, -0.7309, -0.9407,\n",
       "                       0.8926, -1.3090, -0.8112,  0.6735, -0.7722, -1.3697, -0.7544,\n",
       "                       1.0743,  1.5304,  1.4486, -1.0604, -0.6614,  0.2102,  1.7119,\n",
       "                       1.4446, -0.8488, -1.1972, -1.8893, -0.4017, -0.4829,  1.3083,\n",
       "                       1.0836, -1.5890, -0.2127,  1.0742, -0.2562, -1.5217,  0.3588,\n",
       "                      -1.2009,  1.2818, -1.4474, -1.2169, -0.4000,  1.3599,  1.7833,\n",
       "                       0.9570, -0.5710,  0.4007,  0.0744,  0.0706,  0.1190,  0.3176,\n",
       "                       1.1248, -0.6395, -0.9588, -0.8709,  1.1888, -0.9069, -1.1781,\n",
       "                      -0.8922,  1.7925,  1.0113, -0.8882,  1.3649, -0.2139,  0.7986,\n",
       "                       0.2530, -1.0754,  1.1563, -0.1707, -0.8832,  0.4490, -1.2014,\n",
       "                       0.4467, -1.3346, -1.3749,  0.7060, -2.7703, -1.7270, -0.4251,\n",
       "                       0.1126, -1.8608, -2.5030, -1.6292, -1.8886, -0.1196, -1.2121,\n",
       "                      -1.1265, -0.6390, -1.9251, -1.2612, -0.6907, -2.2288, -1.7791,\n",
       "                      -1.7050, -0.6201, -2.8140, -1.8854,  0.7267, -1.6463, -1.6935,\n",
       "                      -1.0627, -0.8754, -1.5620, -0.6452, -2.9258, -0.5663, -2.3230,\n",
       "                      -0.3078, -2.8250, -1.9509, -1.7657, -2.6720, -0.1047, -2.0093,\n",
       "                      -1.9252, -1.3313, -1.5546, -2.0423,  0.0135, -0.5282,  1.3577,\n",
       "                      -1.9627, -0.6436, -1.2796, -2.7830, -1.1334, -0.3869, -1.5235,\n",
       "                      -0.6544, -1.3981, -2.1400, -0.9185, -1.8668, -3.6721, -3.2972,\n",
       "                      -1.4407, -1.6381, -1.8742, -3.4136, -0.7557, -0.7355,  0.1168,\n",
       "                      -1.5879, -2.1513, -1.9841, -1.9063, -2.6081, -3.0016, -1.0724,\n",
       "                      -1.6180,  0.5582, -2.7447, -1.3945, -2.0942, -2.0914, -1.0179,\n",
       "                      -1.0616, -3.1113, -1.0394, -3.5438, -0.7557, -2.8756, -0.5336,\n",
       "                      -2.4766, -0.3584, -2.5090, -3.8061, -1.0999, -1.5480, -2.2850,\n",
       "                      -2.4039, -2.5702, -1.2012, -1.0112, -1.4415, -0.5966, -1.3911,\n",
       "                      -1.2842, -0.8893, -2.5420, -1.5262, -1.1195, -1.7759, -2.3023,\n",
       "                      -1.3368,  0.0230, -1.6429, -2.4655, -1.7453, -1.8772, -1.5280,\n",
       "                      -1.0304, -1.5237,  0.3462, -0.4884, -1.4165, -0.5040, -1.1604,\n",
       "                      -0.4604])),\n",
       "             ('lstm.weight_ih_l1',\n",
       "              tensor([[ 1.3872e+00, -1.6014e+00,  1.2035e-01,  ...,  1.3805e+00,\n",
       "                       -1.1516e+00, -2.9204e+00],\n",
       "                      [ 2.2622e-02, -1.8526e-01, -1.9890e-01,  ..., -5.9500e-01,\n",
       "                       -3.9454e-01, -2.1776e-01],\n",
       "                      [ 9.1439e-01,  3.8203e-01, -3.2925e-01,  ...,  3.4901e-01,\n",
       "                        4.9576e-02, -1.0492e-01],\n",
       "                      ...,\n",
       "                      [-4.2452e-01, -7.7246e-01,  7.3436e-01,  ...,  1.1158e+00,\n",
       "                       -3.4707e-02,  1.5357e-01],\n",
       "                      [ 8.9713e-01,  1.5493e+00,  2.3460e-01,  ...,  5.6812e-01,\n",
       "                       -8.2998e-01,  8.9591e-01],\n",
       "                      [-2.7496e-01, -3.0594e-01, -2.5121e-01,  ...,  3.6676e-01,\n",
       "                       -7.2064e-01,  6.2692e-01]])),\n",
       "             ('lstm.weight_hh_l1',\n",
       "              tensor([[-4.9985e-02,  8.4041e-02,  1.9476e+00,  ...,  9.5697e-01,\n",
       "                       -5.3307e-01, -1.5687e-01],\n",
       "                      [ 6.4314e-01,  1.0951e-01, -2.2693e+00,  ...,  1.2748e-01,\n",
       "                       -5.8060e-01,  9.5404e-02],\n",
       "                      [-2.2312e+00, -1.1720e-01,  7.0395e-01,  ...,  3.9768e-01,\n",
       "                       -5.1986e-01, -3.6867e-02],\n",
       "                      ...,\n",
       "                      [-5.1018e-01, -6.8800e-01, -1.9301e+00,  ...,  1.5831e+00,\n",
       "                        8.8403e-02,  3.7047e-01],\n",
       "                      [ 1.4947e+00,  1.7649e-01, -3.6182e-01,  ...,  5.7337e-01,\n",
       "                       -2.2367e+00, -1.2895e-01],\n",
       "                      [ 5.3937e-01, -2.9167e-01, -1.7758e+00,  ...,  8.8302e-01,\n",
       "                        5.9950e-01, -2.2136e-01]])),\n",
       "             ('lstm.bias_ih_l1',\n",
       "              tensor([ 2.7519, -0.0395, -0.1913, -1.0819, -0.1684,  1.0991, -1.4468,\n",
       "                       0.6416, -0.4405,  0.1344,  0.0388, -1.2092, -0.0513, -0.8508,\n",
       "                      -0.3442, -0.9295, -1.3057,  0.0381, -0.8188, -0.2528, -2.6003,\n",
       "                      -1.0779, -1.8096, -0.0269,  0.2685, -1.1245, -0.3523,  0.1106,\n",
       "                      -0.4285, -1.6032,  0.6489, -0.6464, -0.4067, -1.1698, -0.3935,\n",
       "                      -0.7647, -0.5654, -0.2947, -0.7828, -0.8091,  1.8216, -0.7624,\n",
       "                       0.8441,  0.9576, -0.7077, -0.8960, -0.4563, -0.5191, -0.8936,\n",
       "                      -0.3867, -0.6153,  0.1308,  0.0852, -0.0181,  0.0897, -1.2221,\n",
       "                      -1.3682,  0.1801, -0.7419, -0.4114,  0.6966, -0.9946, -1.0704,\n",
       "                       0.1179, -1.7156,  0.2572, -0.5486, -1.1652, -0.0227, -0.4557,\n",
       "                      -1.0723, -0.2666, -0.2973, -1.5274,  1.0710, -0.9869, -0.6341,\n",
       "                      -1.6518, -0.2768, -0.3822, -0.3911,  0.0370, -0.2707, -0.4816,\n",
       "                      -0.3091, -0.8825, -1.2709, -1.5045, -0.6193, -1.1189, -1.1899,\n",
       "                      -1.7975, -0.8709, -1.5822, -1.1444,  0.7805, -0.0091, -0.4091,\n",
       "                      -0.7129, -0.6805, -1.0199, -0.0902, -1.1391, -0.7398, -0.1448,\n",
       "                       0.2181, -0.1152, -0.4880,  0.8528, -0.5383, -0.6650, -0.5296,\n",
       "                      -0.0919,  1.7517, -0.2237, -0.5763,  1.5198, -0.9477,  0.3249,\n",
       "                       1.0038,  0.1566, -1.5037,  0.2198, -1.8797, -0.5745,  0.0807,\n",
       "                      -0.5743, -0.3690, -2.0650,  0.0421, -0.8080,  0.5430,  0.5532,\n",
       "                       0.1171,  0.2747,  0.7699, -0.4378,  0.5815,  0.0638, -0.4719,\n",
       "                      -0.1829, -1.4305, -0.5506,  0.4010, -1.0064,  0.1704, -1.7155,\n",
       "                       0.3286, -0.2797, -0.2307,  0.3410,  0.7492,  0.1674,  0.2358,\n",
       "                       0.2939,  0.8057, -0.0942, -0.2031,  0.7066, -0.1731, -0.0560,\n",
       "                      -0.9988,  0.4319,  0.7582, -0.1856,  0.0101, -0.0129,  0.3244,\n",
       "                       2.2806, -0.9440,  0.3506,  0.2041, -1.9376,  0.2737,  0.1611,\n",
       "                       0.2022,  0.5265,  0.4649, -0.0807,  0.1094,  0.2280,  0.1715,\n",
       "                       0.0080, -0.3837,  0.0234, -1.8615, -1.3490,  0.0287,  0.4727,\n",
       "                      -2.7258,  0.6966,  0.2654, -0.2898,  0.0213,  0.4647,  0.1236,\n",
       "                       0.0490, -0.0984, -0.3794, -0.9651, -1.0784, -0.5960, -1.5100,\n",
       "                      -0.8973, -1.8482, -0.0215, -1.3226, -1.6484, -1.5334,  0.0537,\n",
       "                       0.3328, -0.7575, -0.7203, -0.1040,  0.0538, -2.1500,  0.3096,\n",
       "                      -0.0003, -0.0033,  0.1953, -1.7057, -0.8468, -0.1783, -0.7431,\n",
       "                      -0.7075, -1.1239,  0.0075, -0.1220, -0.8088, -0.1610, -1.0925,\n",
       "                      -0.0914, -0.3830, -0.4237, -0.0866, -0.3270, -4.7906, -0.2230,\n",
       "                      -0.0504, -0.8425,  0.1048,  0.5721, -0.2288, -0.3148,  0.1739,\n",
       "                      -0.8243, -0.0325, -0.5285, -3.2296, -0.2278,  0.2940, -0.4615,\n",
       "                      -2.3842, -0.3497, -1.7336, -0.1883, -2.1421, -0.9254,  0.6800,\n",
       "                       0.6926, -1.3369,  1.2532, -0.5516, -0.9842, -1.2727,  0.8908,\n",
       "                      -0.7334, -0.7453,  0.5286, -1.5334, -1.1659,  0.9930,  0.2205,\n",
       "                       0.6968,  1.0920,  0.9804,  0.3051, -0.2521, -0.6595, -0.1931,\n",
       "                       0.9099,  0.4608, -0.6666, -0.7187,  0.7123, -0.7930, -1.6240,\n",
       "                      -0.7184, -0.7652,  1.7181,  1.1569, -0.8414, -0.8311,  0.5282,\n",
       "                      -0.2734,  1.1951,  1.3051, -1.6892, -1.0370,  0.9163,  1.1090,\n",
       "                       1.1064, -0.8508, -1.1974, -0.4026, -0.6054,  0.5537, -0.9057,\n",
       "                      -1.1086,  1.1456, -0.7330,  0.2580, -0.6134,  1.3999,  1.6230,\n",
       "                      -0.6666,  1.1596, -1.1346,  0.8732,  0.8192,  0.7346, -0.8717,\n",
       "                       1.0286, -0.6125,  0.8737,  0.4505, -0.7194,  1.1287, -1.0594,\n",
       "                      -0.6416,  1.4000,  1.0360,  0.9820,  1.1250,  1.3166, -1.0210,\n",
       "                       1.0916, -0.7217,  0.6699, -0.6131, -0.8069,  0.0972,  0.9506,\n",
       "                       0.5207, -1.1402,  0.5693, -1.1239,  0.1394,  1.6731, -1.1273,\n",
       "                       0.1638, -2.1765, -1.0123,  1.3333,  0.4355, -0.5282, -0.3470,\n",
       "                       0.8111, -0.2314, -0.6109, -0.9306, -1.6357,  0.8605, -0.7623,\n",
       "                      -1.6919,  0.8922,  0.4470, -0.4512,  0.9237, -1.3482, -0.7354,\n",
       "                      -0.9797, -1.4803,  1.0237,  1.0035, -1.2776, -1.6629,  0.1040,\n",
       "                       1.2748, -0.4033,  1.2036, -0.7663,  1.3528,  0.6719,  2.6068,\n",
       "                      -0.4987, -2.3501, -2.1603, -0.4576,  1.9955, -1.3382, -0.7238,\n",
       "                      -2.7509, -2.9583, -0.9046, -2.7171, -1.2191, -0.4613, -1.5999,\n",
       "                      -0.9938, -2.0451, -0.7924, -1.3122, -2.0336, -1.8733, -1.0379,\n",
       "                      -1.8359, -2.5390, -0.9601, -1.7096, -1.5369, -1.6098, -3.1042,\n",
       "                      -0.6106, -0.4381, -1.9195, -2.1080, -1.3544, -2.6342, -0.4947,\n",
       "                      -0.8207, -1.4559, -1.5074, -0.8572,  1.4829,  0.6331, -3.2401,\n",
       "                      -2.0354, -0.8459, -0.5339, -1.4388, -1.0191, -2.8769, -2.0263,\n",
       "                      -0.9813, -1.4727, -1.7338, -2.4220, -1.4575, -2.0664, -1.3944,\n",
       "                       0.9030, -1.7973, -1.6633, -2.4108, -1.7018, -3.1911, -0.6519,\n",
       "                      -1.5442, -1.2338, -3.4275, -2.8235, -1.7237, -0.5965, -0.2588,\n",
       "                      -1.1443, -2.3506, -1.7853, -1.1342, -1.0531, -0.0908, -2.6405,\n",
       "                      -2.0274, -1.3045, -1.6754, -1.5949, -1.7002, -2.1240, -2.0783,\n",
       "                      -1.0559, -1.6791, -0.5787, -1.8792, -0.9128, -0.2421, -1.8227,\n",
       "                      -1.2026, -1.9808, -1.9554, -2.4393, -0.6069, -1.7389, -0.9015,\n",
       "                      -1.0148, -1.1654, -1.2705, -1.5257, -0.9930, -2.6994,  1.5434,\n",
       "                      -0.7488, -1.0962,  2.6135, -1.7821, -1.5514, -2.8545, -1.3548,\n",
       "                       2.4664, -2.9957, -2.4764,  1.4390, -2.0463, -2.7388,  4.3476,\n",
       "                       0.2612, -1.1391, -1.6578, -1.0317, -2.4956, -0.9760, -0.8066,\n",
       "                      -1.4508])),\n",
       "             ('lstm.bias_hh_l1',\n",
       "              tensor([ 2.7519, -0.0395, -0.1913, -1.0819, -0.1684,  1.0991, -1.4468,\n",
       "                       0.6416, -0.4405,  0.1344,  0.0388, -1.2092, -0.0513, -0.8508,\n",
       "                      -0.3442, -0.9295, -1.3057,  0.0381, -0.8188, -0.2528, -2.6003,\n",
       "                      -1.0779, -1.8096, -0.0269,  0.2685, -1.1245, -0.3523,  0.1106,\n",
       "                      -0.4285, -1.6032,  0.6489, -0.6464, -0.4067, -1.1698, -0.3935,\n",
       "                      -0.7647, -0.5654, -0.2947, -0.7828, -0.8091,  1.8216, -0.7624,\n",
       "                       0.8441,  0.9576, -0.7077, -0.8960, -0.4563, -0.5191, -0.8936,\n",
       "                      -0.3867, -0.6153,  0.1308,  0.0852, -0.0181,  0.0897, -1.2221,\n",
       "                      -1.3682,  0.1801, -0.7419, -0.4114,  0.6966, -0.9946, -1.0704,\n",
       "                       0.1179, -1.7156,  0.2572, -0.5486, -1.1652, -0.0227, -0.4557,\n",
       "                      -1.0723, -0.2666, -0.2973, -1.5274,  1.0710, -0.9869, -0.6341,\n",
       "                      -1.6518, -0.2768, -0.3822, -0.3911,  0.0370, -0.2707, -0.4816,\n",
       "                      -0.3091, -0.8825, -1.2709, -1.5045, -0.6193, -1.1189, -1.1899,\n",
       "                      -1.7975, -0.8709, -1.5822, -1.1444,  0.7805, -0.0091, -0.4091,\n",
       "                      -0.7129, -0.6805, -1.0199, -0.0902, -1.1391, -0.7398, -0.1448,\n",
       "                       0.2181, -0.1152, -0.4880,  0.8528, -0.5383, -0.6650, -0.5296,\n",
       "                      -0.0919,  1.7517, -0.2237, -0.5763,  1.5198, -0.9477,  0.3249,\n",
       "                       1.0038,  0.1566, -1.5037,  0.2198, -1.8797, -0.5745,  0.0807,\n",
       "                      -0.5743, -0.3690, -2.0650,  0.0421, -0.8080,  0.5430,  0.5532,\n",
       "                       0.1171,  0.2747,  0.7699, -0.4378,  0.5815,  0.0638, -0.4719,\n",
       "                      -0.1829, -1.4305, -0.5506,  0.4010, -1.0064,  0.1704, -1.7155,\n",
       "                       0.3286, -0.2797, -0.2307,  0.3410,  0.7492,  0.1674,  0.2358,\n",
       "                       0.2939,  0.8057, -0.0942, -0.2031,  0.7066, -0.1731, -0.0560,\n",
       "                      -0.9988,  0.4319,  0.7582, -0.1856,  0.0101, -0.0129,  0.3244,\n",
       "                       2.2806, -0.9440,  0.3506,  0.2041, -1.9376,  0.2737,  0.1611,\n",
       "                       0.2022,  0.5265,  0.4649, -0.0807,  0.1094,  0.2280,  0.1715,\n",
       "                       0.0080, -0.3837,  0.0234, -1.8615, -1.3490,  0.0287,  0.4727,\n",
       "                      -2.7258,  0.6966,  0.2654, -0.2898,  0.0213,  0.4647,  0.1236,\n",
       "                       0.0490, -0.0984, -0.3794, -0.9651, -1.0784, -0.5960, -1.5100,\n",
       "                      -0.8973, -1.8482, -0.0215, -1.3226, -1.6484, -1.5334,  0.0537,\n",
       "                       0.3328, -0.7575, -0.7203, -0.1040,  0.0538, -2.1500,  0.3096,\n",
       "                      -0.0003, -0.0033,  0.1953, -1.7057, -0.8468, -0.1783, -0.7431,\n",
       "                      -0.7075, -1.1239,  0.0075, -0.1220, -0.8088, -0.1610, -1.0925,\n",
       "                      -0.0914, -0.3830, -0.4237, -0.0866, -0.3270, -4.7906, -0.2230,\n",
       "                      -0.0504, -0.8425,  0.1048,  0.5721, -0.2288, -0.3148,  0.1739,\n",
       "                      -0.8243, -0.0325, -0.5285, -3.2296, -0.2278,  0.2940, -0.4615,\n",
       "                      -2.3842, -0.3497, -1.7336, -0.1883, -2.1421, -0.9254,  0.6800,\n",
       "                       0.6926, -1.3369,  1.2532, -0.5516, -0.9842, -1.2727,  0.8908,\n",
       "                      -0.7334, -0.7453,  0.5286, -1.5334, -1.1659,  0.9930,  0.2205,\n",
       "                       0.6968,  1.0920,  0.9804,  0.3051, -0.2521, -0.6595, -0.1931,\n",
       "                       0.9099,  0.4608, -0.6666, -0.7187,  0.7123, -0.7930, -1.6240,\n",
       "                      -0.7184, -0.7652,  1.7181,  1.1569, -0.8414, -0.8311,  0.5282,\n",
       "                      -0.2734,  1.1951,  1.3051, -1.6892, -1.0370,  0.9163,  1.1090,\n",
       "                       1.1064, -0.8508, -1.1974, -0.4026, -0.6054,  0.5537, -0.9057,\n",
       "                      -1.1086,  1.1456, -0.7330,  0.2580, -0.6134,  1.3999,  1.6230,\n",
       "                      -0.6666,  1.1596, -1.1346,  0.8732,  0.8192,  0.7346, -0.8717,\n",
       "                       1.0286, -0.6125,  0.8737,  0.4505, -0.7194,  1.1287, -1.0594,\n",
       "                      -0.6416,  1.4000,  1.0360,  0.9820,  1.1250,  1.3166, -1.0210,\n",
       "                       1.0916, -0.7217,  0.6699, -0.6131, -0.8069,  0.0972,  0.9506,\n",
       "                       0.5207, -1.1402,  0.5693, -1.1239,  0.1394,  1.6731, -1.1273,\n",
       "                       0.1638, -2.1765, -1.0123,  1.3333,  0.4355, -0.5282, -0.3470,\n",
       "                       0.8111, -0.2314, -0.6109, -0.9306, -1.6357,  0.8605, -0.7623,\n",
       "                      -1.6919,  0.8922,  0.4470, -0.4512,  0.9237, -1.3482, -0.7354,\n",
       "                      -0.9797, -1.4803,  1.0237,  1.0035, -1.2776, -1.6629,  0.1040,\n",
       "                       1.2748, -0.4033,  1.2036, -0.7663,  1.3528,  0.6719,  2.6068,\n",
       "                      -0.4987, -2.3501, -2.1603, -0.4576,  1.9955, -1.3382, -0.7238,\n",
       "                      -2.7509, -2.9583, -0.9046, -2.7171, -1.2191, -0.4613, -1.5999,\n",
       "                      -0.9938, -2.0451, -0.7924, -1.3122, -2.0336, -1.8733, -1.0379,\n",
       "                      -1.8359, -2.5390, -0.9601, -1.7096, -1.5369, -1.6098, -3.1042,\n",
       "                      -0.6106, -0.4381, -1.9195, -2.1080, -1.3544, -2.6342, -0.4947,\n",
       "                      -0.8207, -1.4559, -1.5074, -0.8572,  1.4829,  0.6331, -3.2401,\n",
       "                      -2.0354, -0.8459, -0.5339, -1.4388, -1.0191, -2.8769, -2.0263,\n",
       "                      -0.9813, -1.4727, -1.7338, -2.4220, -1.4575, -2.0664, -1.3944,\n",
       "                       0.9030, -1.7973, -1.6633, -2.4108, -1.7018, -3.1911, -0.6519,\n",
       "                      -1.5442, -1.2338, -3.4275, -2.8235, -1.7237, -0.5965, -0.2588,\n",
       "                      -1.1443, -2.3506, -1.7853, -1.1342, -1.0531, -0.0908, -2.6405,\n",
       "                      -2.0274, -1.3045, -1.6754, -1.5949, -1.7002, -2.1240, -2.0783,\n",
       "                      -1.0559, -1.6791, -0.5787, -1.8792, -0.9128, -0.2421, -1.8227,\n",
       "                      -1.2026, -1.9808, -1.9554, -2.4393, -0.6069, -1.7389, -0.9015,\n",
       "                      -1.0148, -1.1654, -1.2705, -1.5257, -0.9930, -2.6994,  1.5434,\n",
       "                      -0.7488, -1.0962,  2.6135, -1.7821, -1.5514, -2.8545, -1.3548,\n",
       "                       2.4664, -2.9957, -2.4764,  1.4390, -2.0463, -2.7388,  4.3476,\n",
       "                       0.2612, -1.1391, -1.6578, -1.0317, -2.4956, -0.9760, -0.8066,\n",
       "                      -1.4508])),\n",
       "             ('decoder.weight',\n",
       "              tensor([[ 5.8332e-01, -1.4337e+00, -2.2049e+00,  ..., -4.4479e-01,\n",
       "                        7.4530e-03, -2.3712e-02],\n",
       "                      [ 5.6085e-02,  7.3383e-02, -5.0731e-01,  ...,  8.9408e-02,\n",
       "                       -1.8396e-02, -7.6969e-02],\n",
       "                      [-3.6358e-01, -1.1261e+00,  1.0747e+00,  ..., -3.1931e-01,\n",
       "                        9.2002e-01, -1.7504e-01],\n",
       "                      ...,\n",
       "                      [ 1.7477e+00, -9.6287e-01, -1.8131e+00,  ..., -3.5160e-01,\n",
       "                        1.7681e+00,  9.1334e-01],\n",
       "                      [ 1.9360e-01,  6.9693e-01,  5.2567e-01,  ...,  6.3451e-01,\n",
       "                       -5.2049e-01,  1.1376e-01],\n",
       "                      [ 1.3177e+00,  3.8998e-01,  6.4989e-01,  ...,  3.0259e-01,\n",
       "                        6.0718e-01, -1.5447e-01]])),\n",
       "             ('decoder.bias',\n",
       "              tensor([ 0.2697, -2.2598, -2.3005,  4.4114, -0.7468, -2.1602,  3.9473,\n",
       "                      -1.7249,  0.9526,  1.8132,  2.0133,  1.6177,  0.6128,  0.2569,\n",
       "                      -3.5583,  2.1954, -2.5229,  2.5418,  1.4696, -1.7433,  1.3382,\n",
       "                       2.9689, -1.8597, -0.2569, -1.6889, -0.8054,  1.0943, -1.7545,\n",
       "                       0.2485, -1.9844, -0.7526, -0.6149,  0.1826,  0.5992,  2.0561,\n",
       "                      -1.1476,  4.7457,  2.7465, -2.2984, -2.2968,  1.5136, -0.2942,\n",
       "                       2.2158,  0.5137, -2.1641, -1.1263,  3.7644, -1.5105, -1.8064,\n",
       "                      -2.5753, -2.1314,  4.0946, -2.1222, -2.0921, -2.2522,  1.6897,\n",
       "                       3.9092,  2.2464,  1.4939,  0.6340, -1.6811, -2.0600,  0.7704,\n",
       "                       1.2089, -2.8242,  2.0434,  1.0091, -0.8185, -1.0233, -5.4232,\n",
       "                       0.0533,  0.4535, -2.4496,  4.1874, -2.5725, -1.5527, -2.1801,\n",
       "                      -3.2613, -1.4833,  2.8794, -2.1152,  2.5513,  4.4407, -1.7853,\n",
       "                       2.2802]))])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(os.path.join(checkpoints_dir, model_name), 'cpu')\n",
    "# model.load_state_dict("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize dataset with 1735414 characters, 85 unique.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TextGenerationModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-211-b02eecdf4d70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                     seq_length=30)\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m model = TextGenerationModel(batch_size=64,\n\u001b[0m\u001b[1;32m      5\u001b[0m                             \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                             \u001b[0mvocabulary_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TextGenerationModel' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = TextDataset(filename='../part3/books/vanity_fair.txt',\n",
    "                    seq_length=30)\n",
    "\n",
    "model = TextGenerationModel(batch_size=64,\n",
    "                            seq_length=30,\n",
    "                            vocabulary_size=dataset.vocab_size,\n",
    "                            lstm_num_hidden=config.lstm_num_hidden,\n",
    "                            lstm_num_layers=config.lstm_num_layers,\n",
    "                            dropout=dropout, \n",
    "                            embedding=config.embedding,\n",
    "                            device=config.device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76660"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3833 * 20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
